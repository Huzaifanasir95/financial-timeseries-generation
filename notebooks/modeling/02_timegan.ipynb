{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175a271b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bdd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import ks_2samp, pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÖ TimeGAN Implementation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üî¢ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üé≤ Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9c1a8",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "PROCESSED_DATA_DIR = '../../data/processed'\n",
    "FEATURE_DATA_DIR = '../../data/features'\n",
    "MODELS_DIR = '../../models/timegan'\n",
    "RESULTS_DIR = '../../outputs/results'\n",
    "FIGURES_DIR = '../../outputs/figures'\n",
    "SYNTHETIC_DATA_DIR = '../../data/synthetic/timegan'\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(SYNTHETIC_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# TimeGAN Hyperparameters\n",
    "CONFIG = {\n",
    "    # Data parameters\n",
    "    'seq_len': 24,           # Sequence length (24 trading days ~ 1 month)\n",
    "    'n_features': None,      # Will be set based on data\n",
    "    'batch_size': 128,\n",
    "    \n",
    "    # Network architecture\n",
    "    'hidden_dim': 64,        # Hidden dimension for RNN layers\n",
    "    'num_layers': 3,         # Number of RNN layers\n",
    "    'z_dim': 64,             # Random noise dimension\n",
    "    \n",
    "    # Training parameters\n",
    "    'iterations': 10000,     # Total training iterations\n",
    "    'learning_rate': 1e-3,\n",
    "    \n",
    "    # Loss weights\n",
    "    'gamma': 1.0,            # Supervised loss weight\n",
    "}\n",
    "\n",
    "# Assets to generate synthetic data for\n",
    "TEST_ASSETS = ['GSPC', 'IXIC', 'AAPL', 'BTC_USD']\n",
    "\n",
    "print(f\"üìÇ Models directory: {MODELS_DIR}\")\n",
    "print(f\"üìÇ Synthetic data directory: {SYNTHETIC_DATA_DIR}\")\n",
    "print(f\"\\n‚öôÔ∏è  TimeGAN Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key:20s}: {value}\")\n",
    "print(f\"\\nüéØ Assets for synthetic generation: {', '.join(TEST_ASSETS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523460b",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(ticker, use_features=True):\n",
    "    \"\"\"\n",
    "    Load and prepare data for TimeGAN training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Asset ticker symbol\n",
    "    use_features : bool\n",
    "        If True, use engineered features; if False, use only returns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data_scaled : np.ndarray\n",
    "        Normalized data ready for TimeGAN\n",
    "    scaler : MinMaxScaler\n",
    "        Fitted scaler for inverse transform\n",
    "    feature_names : list\n",
    "        Names of features used\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì• Loading data for {ticker}...\")\n",
    "    \n",
    "    if use_features:\n",
    "        # Load engineered features\n",
    "        data = pd.read_csv(os.path.join(FEATURE_DATA_DIR, f\"{ticker}_features.csv\"), \n",
    "                          index_col=0, parse_dates=True)\n",
    "        \n",
    "        # Select important features (exclude highly correlated ones)\n",
    "        feature_cols = [\n",
    "            'Returns', 'Log_Returns', 'Volume_Change',\n",
    "            # Momentum\n",
    "            'RSI', 'MACD', 'MACD_Signal', 'Stochastic_K',\n",
    "            # Trend  \n",
    "            'SMA_20', 'EMA_20', 'ADX',\n",
    "            # Volatility\n",
    "            'BB_Width', 'ATR', 'Historical_Vol',\n",
    "            # Lagged features\n",
    "            'Returns_Lag_1', 'Returns_Lag_5',\n",
    "            # Rolling statistics\n",
    "            'Returns_Rolling_Mean_5', 'Returns_Rolling_Std_5',\n",
    "            'Returns_Rolling_Mean_20', 'Returns_Rolling_Std_20'\n",
    "        ]\n",
    "        \n",
    "        # Filter columns that exist\n",
    "        feature_cols = [col for col in feature_cols if col in data.columns]\n",
    "        data = data[feature_cols].dropna()\n",
    "        \n",
    "    else:\n",
    "        # Use only returns data\n",
    "        train = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'train', f\"{ticker}_train.csv\"), \n",
    "                           index_col=0, parse_dates=True)\n",
    "        data = train[['Returns']].dropna()\n",
    "        feature_cols = ['Returns']\n",
    "    \n",
    "    print(f\"   Shape: {data.shape}\")\n",
    "    print(f\"   Features: {len(feature_cols)}\")\n",
    "    print(f\"   Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "    \n",
    "    # Normalize data to [0, 1]\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data.values)\n",
    "    \n",
    "    print(f\"   ‚úÖ Data normalized to [{data_scaled.min():.3f}, {data_scaled.max():.3f}]\")\n",
    "    \n",
    "    return data_scaled, scaler, feature_cols\n",
    "\n",
    "print(\"‚úÖ Data loading function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61acde1f",
   "metadata": {},
   "source": [
    "## 4. Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_len):\n",
    "    \"\"\"\n",
    "    Create overlapping sequences for TimeGAN training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        Input data of shape (n_samples, n_features)\n",
    "    seq_len : int\n",
    "        Length of each sequence\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sequences : np.ndarray\n",
    "        Array of shape (n_sequences, seq_len, n_features)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        sequences.append(data[i:i + seq_len])\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    \n",
    "    print(f\"\\nüì¶ Created {len(sequences):,} sequences\")\n",
    "    print(f\"   Sequence shape: {sequences.shape}\")\n",
    "    print(f\"   Total parameters per sequence: {seq_len} timesteps √ó {sequences.shape[2]} features\")\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "print(\"‚úÖ Sequence creation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9246d6",
   "metadata": {},
   "source": [
    "## 5. TimeGAN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c77624",
   "metadata": {},
   "source": [
    "### 5.1 Embedding Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e600970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedder(seq_len, n_features, hidden_dim, num_layers):\n",
    "    \"\"\"\n",
    "    Build the embedding network (encoder).\n",
    "    Maps input sequences to latent representations.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_len, n_features), name='embedder_input')\n",
    "    x = inputs\n",
    "    \n",
    "    # Stack GRU layers\n",
    "    for i in range(num_layers):\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        x = layers.GRU(\n",
    "            hidden_dim,\n",
    "            return_sequences=True,  # Always return sequences for TimeGAN\n",
    "            name=f'embedder_gru_{i+1}'\n",
    "        )(x)\n",
    "    \n",
    "    # Output: latent representation\n",
    "    outputs = layers.Dense(hidden_dim, activation='sigmoid', name='embedder_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Embedder')\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Embedder architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c496323",
   "metadata": {},
   "source": [
    "### 5.2 Recovery Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recovery(seq_len, n_features, hidden_dim, num_layers):\n",
    "    \"\"\"\n",
    "    Build the recovery network (decoder).\n",
    "    Reconstructs original features from latent representations.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_len, hidden_dim), name='recovery_input')\n",
    "    x = inputs\n",
    "    \n",
    "    # Stack GRU layers\n",
    "    for i in range(num_layers):\n",
    "        x = layers.GRU(\n",
    "            hidden_dim,\n",
    "            return_sequences=True,\n",
    "            name=f'recovery_gru_{i+1}'\n",
    "        )(x)\n",
    "    \n",
    "    # Output: reconstructed features\n",
    "    outputs = layers.Dense(n_features, activation='sigmoid', name='recovery_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Recovery')\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Recovery architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f8882",
   "metadata": {},
   "source": [
    "### 5.3 Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f55c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(seq_len, z_dim, hidden_dim, num_layers):\n",
    "    \"\"\"\n",
    "    Build the generator network.\n",
    "    Generates synthetic latent representations from random noise.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_len, z_dim), name='generator_input')\n",
    "    x = inputs\n",
    "    \n",
    "    # Stack GRU layers\n",
    "    for i in range(num_layers):\n",
    "        x = layers.GRU(\n",
    "            hidden_dim,\n",
    "            return_sequences=True,\n",
    "            name=f'generator_gru_{i+1}'\n",
    "        )(x)\n",
    "    \n",
    "    # Output: synthetic latent representation\n",
    "    outputs = layers.Dense(hidden_dim, activation='sigmoid', name='generator_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Generator architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e06d04d",
   "metadata": {},
   "source": [
    "### 5.4 Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93014e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(seq_len, hidden_dim, num_layers):\n",
    "    \"\"\"\n",
    "    Build the discriminator network.\n",
    "    Distinguishes real from synthetic latent representations.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_len, hidden_dim), name='discriminator_input')\n",
    "    x = inputs\n",
    "    \n",
    "    # Stack GRU layers\n",
    "    for i in range(num_layers):\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        x = layers.GRU(\n",
    "            hidden_dim,\n",
    "            return_sequences=return_sequences,\n",
    "            name=f'discriminator_gru_{i+1}'\n",
    "        )(x)\n",
    "    \n",
    "    # Output: probability of being real\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name='discriminator_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Discriminator')\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Discriminator architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4d738",
   "metadata": {},
   "source": [
    "### 5.5 Supervisor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_supervisor(seq_len, hidden_dim, num_layers):\n",
    "    \"\"\"\n",
    "    Build the supervisor network.\n",
    "    Ensures temporal consistency in latent space by predicting next timestep.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_len - 1, hidden_dim), name='supervisor_input')\n",
    "    x = inputs\n",
    "    \n",
    "    # Stack GRU layers (fewer than other networks)\n",
    "    for i in range(num_layers - 1):\n",
    "        x = layers.GRU(\n",
    "            hidden_dim,\n",
    "            return_sequences=True,\n",
    "            name=f'supervisor_gru_{i+1}'\n",
    "        )(x)\n",
    "    \n",
    "    # Output: predicted next latent representation\n",
    "    outputs = layers.Dense(hidden_dim, activation='sigmoid', name='supervisor_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Supervisor')\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Supervisor architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214a14f",
   "metadata": {},
   "source": [
    "## 6. TimeGAN Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGAN:\n",
    "    \"\"\"\n",
    "    TimeGAN implementation for financial time-series generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.seq_len = config['seq_len']\n",
    "        self.n_features = config['n_features']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.z_dim = config['z_dim']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.gamma = config['gamma']\n",
    "        \n",
    "        # Build networks\n",
    "        self._build_networks()\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.embedder_optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.recovery_optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.supervisor_optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.generator_optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.discriminator_optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # Loss functions\n",
    "        self.mse_loss = MeanSquaredError()\n",
    "        self.bce_loss = BinaryCrossentropy()\n",
    "        \n",
    "        print(\"\\n‚úÖ TimeGAN initialized\")\n",
    "        self._print_model_summary()\n",
    "    \n",
    "    def _build_networks(self):\n",
    "        \"\"\"Build all TimeGAN networks.\"\"\"\n",
    "        print(\"\\nüèóÔ∏è  Building TimeGAN networks...\")\n",
    "        \n",
    "        self.embedder = build_embedder(\n",
    "            self.seq_len, self.n_features, self.hidden_dim, self.num_layers\n",
    "        )\n",
    "        \n",
    "        self.recovery = build_recovery(\n",
    "            self.seq_len, self.n_features, self.hidden_dim, self.num_layers\n",
    "        )\n",
    "        \n",
    "        self.generator = build_generator(\n",
    "            self.seq_len, self.z_dim, self.hidden_dim, self.num_layers\n",
    "        )\n",
    "        \n",
    "        self.discriminator = build_discriminator(\n",
    "            self.seq_len, self.hidden_dim, self.num_layers\n",
    "        )\n",
    "        \n",
    "        self.supervisor = build_supervisor(\n",
    "            self.seq_len, self.hidden_dim, self.num_layers\n",
    "        )\n",
    "    \n",
    "    def _print_model_summary(self):\n",
    "        \"\"\"Print parameter counts for all networks.\"\"\"\n",
    "        print(\"\\nüìä Network Parameters:\")\n",
    "        print(f\"   Embedder:      {self.embedder.count_params():,}\")\n",
    "        print(f\"   Recovery:      {self.recovery.count_params():,}\")\n",
    "        print(f\"   Generator:     {self.generator.count_params():,}\")\n",
    "        print(f\"   Discriminator: {self.discriminator.count_params():,}\")\n",
    "        print(f\"   Supervisor:    {self.supervisor.count_params():,}\")\n",
    "        total = sum([\n",
    "            self.embedder.count_params(),\n",
    "            self.recovery.count_params(),\n",
    "            self.generator.count_params(),\n",
    "            self.discriminator.count_params(),\n",
    "            self.supervisor.count_params()\n",
    "        ])\n",
    "        print(f\"   {'='*30}\")\n",
    "        print(f\"   TOTAL:         {total:,}\")\n",
    "    \n",
    "    def generate_noise(self, batch_size):\n",
    "        \"\"\"Generate random noise for generator input.\"\"\"\n",
    "        return np.random.uniform(0, 1, size=(batch_size, self.seq_len, self.z_dim))\n",
    "    \n",
    "    @tf.function\n",
    "    def train_autoencoder_step(self, X_batch):\n",
    "        \"\"\"Training step for autoencoder (embedder + recovery).\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Embed and reconstruct\n",
    "            H = self.embedder(X_batch, training=True)\n",
    "            X_recon = self.recovery(H, training=True)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            loss = self.mse_loss(X_batch, X_recon)\n",
    "        \n",
    "        # Update weights\n",
    "        variables = self.embedder.trainable_variables + self.recovery.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.embedder_optimizer.apply_gradients(zip(gradients[:len(self.embedder.trainable_variables)], \n",
    "                                                     self.embedder.trainable_variables))\n",
    "        self.recovery_optimizer.apply_gradients(zip(gradients[len(self.embedder.trainable_variables):], \n",
    "                                                     self.recovery.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_supervisor_step(self, X_batch):\n",
    "        \"\"\"Training step for supervisor.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Embed sequences\n",
    "            H = self.embedder(X_batch, training=True)\n",
    "            \n",
    "            # Supervisor predicts next step\n",
    "            H_supervise = self.supervisor(H[:, :-1, :], training=True)\n",
    "            \n",
    "            # Supervised loss\n",
    "            loss = self.mse_loss(H[:, 1:, :], H_supervise)\n",
    "        \n",
    "        # Update supervisor\n",
    "        gradients = tape.gradient(loss, self.supervisor.trainable_variables)\n",
    "        self.supervisor_optimizer.apply_gradients(zip(gradients, self.supervisor.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_generator_step(self, X_batch, Z_batch):\n",
    "        \"\"\"Training step for generator.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate synthetic latent representations\n",
    "            H_fake = self.generator(Z_batch, training=True)\n",
    "            H_supervise = self.supervisor(H_fake[:, :-1, :], training=True)\n",
    "            \n",
    "            # Discriminator's opinion on fake data\n",
    "            Y_fake = self.discriminator(H_fake, training=False)\n",
    "            \n",
    "            # Generator wants discriminator to think fake is real\n",
    "            adversarial_loss = self.bce_loss(tf.ones_like(Y_fake), Y_fake)\n",
    "            \n",
    "            # Supervised loss (temporal consistency)\n",
    "            supervised_loss = self.mse_loss(H_fake[:, 1:, :], H_supervise)\n",
    "            \n",
    "            # Total generator loss\n",
    "            loss = adversarial_loss + self.gamma * supervised_loss\n",
    "        \n",
    "        # Update generator and supervisor\n",
    "        variables = self.generator.trainable_variables + self.supervisor.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients[:len(self.generator.trainable_variables)], \n",
    "                                                      self.generator.trainable_variables))\n",
    "        self.supervisor_optimizer.apply_gradients(zip(gradients[len(self.generator.trainable_variables):], \n",
    "                                                       self.supervisor.trainable_variables))\n",
    "        \n",
    "        return loss, adversarial_loss, supervised_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_discriminator_step(self, X_batch, Z_batch):\n",
    "        \"\"\"Training step for discriminator.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Real data\n",
    "            H_real = self.embedder(X_batch, training=True)\n",
    "            Y_real = self.discriminator(H_real, training=True)\n",
    "            \n",
    "            # Fake data\n",
    "            H_fake = self.generator(Z_batch, training=True)\n",
    "            Y_fake = self.discriminator(H_fake, training=True)\n",
    "            \n",
    "            # Discriminator loss\n",
    "            real_loss = self.bce_loss(tf.ones_like(Y_real), Y_real)\n",
    "            fake_loss = self.bce_loss(tf.zeros_like(Y_fake), Y_fake)\n",
    "            loss = real_loss + fake_loss\n",
    "        \n",
    "        # Update discriminator\n",
    "        gradients = tape.gradient(loss, self.discriminator.trainable_variables)\n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        return loss, real_loss, fake_loss\n",
    "    \n",
    "    def train(self, X_train, iterations):\n",
    "        \"\"\"\n",
    "        Train TimeGAN with three phases:\n",
    "        1. Autoencoder (embedder + recovery)\n",
    "        2. Supervised (supervisor)\n",
    "        3. Joint (generator + discriminator + all)\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        # Phase 1: Autoencoder training\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PHASE 1: AUTOENCODER TRAINING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        ae_iterations = iterations // 4\n",
    "        for step in tqdm(range(ae_iterations), desc=\"Training Autoencoder\"):\n",
    "            # Sample batch\n",
    "            idx = np.random.randint(0, n_samples, self.batch_size)\n",
    "            X_batch = X_train[idx]\n",
    "            \n",
    "            # Train step\n",
    "            loss = self.train_autoencoder_step(X_batch)\n",
    "            \n",
    "            # Log progress\n",
    "            if (step + 1) % 500 == 0:\n",
    "                print(f\"   Step {step+1}/{ae_iterations} - Reconstruction Loss: {loss:.6f}\")\n",
    "        \n",
    "        # Phase 2: Supervisor training\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PHASE 2: SUPERVISOR TRAINING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        sup_iterations = iterations // 4\n",
    "        for step in tqdm(range(sup_iterations), desc=\"Training Supervisor\"):\n",
    "            # Sample batch\n",
    "            idx = np.random.randint(0, n_samples, self.batch_size)\n",
    "            X_batch = X_train[idx]\n",
    "            \n",
    "            # Train step\n",
    "            loss = self.train_supervisor_step(X_batch)\n",
    "            \n",
    "            # Log progress\n",
    "            if (step + 1) % 500 == 0:\n",
    "                print(f\"   Step {step+1}/{sup_iterations} - Supervised Loss: {loss:.6f}\")\n",
    "        \n",
    "        # Phase 3: Joint training\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PHASE 3: JOINT GAN TRAINING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        joint_iterations = iterations // 2\n",
    "        for step in tqdm(range(joint_iterations), desc=\"Joint Training\"):\n",
    "            # Sample batch\n",
    "            idx = np.random.randint(0, n_samples, self.batch_size)\n",
    "            X_batch = X_train[idx]\n",
    "            Z_batch = self.generate_noise(self.batch_size)\n",
    "            \n",
    "            # Train generator (2 steps)\n",
    "            for _ in range(2):\n",
    "                g_loss, g_adv_loss, g_sup_loss = self.train_generator_step(X_batch, Z_batch)\n",
    "            \n",
    "            # Train discriminator (1 step)\n",
    "            d_loss, d_real_loss, d_fake_loss = self.train_discriminator_step(X_batch, Z_batch)\n",
    "            \n",
    "            # Log progress\n",
    "            if (step + 1) % 500 == 0:\n",
    "                print(f\"\\n   Step {step+1}/{joint_iterations}\")\n",
    "                print(f\"      Generator - Total: {g_loss:.6f}, Adversarial: {g_adv_loss:.6f}, Supervised: {g_sup_loss:.6f}\")\n",
    "                print(f\"      Discriminator - Total: {d_loss:.6f}, Real: {d_real_loss:.6f}, Fake: {d_fake_loss:.6f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def generate_samples(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generate synthetic samples.\n",
    "        \"\"\"\n",
    "        # Generate random noise\n",
    "        Z = self.generate_noise(n_samples)\n",
    "        \n",
    "        # Generate latent representations\n",
    "        H_fake = self.generator(Z, training=False)\n",
    "        \n",
    "        # Recover to original feature space\n",
    "        X_fake = self.recovery(H_fake, training=False)\n",
    "        \n",
    "        return X_fake.numpy()\n",
    "    \n",
    "    def save_models(self, save_dir):\n",
    "        \"\"\"Save all trained models.\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        self.embedder.save(os.path.join(save_dir, 'embedder.h5'))\n",
    "        self.recovery.save(os.path.join(save_dir, 'recovery.h5'))\n",
    "        self.generator.save(os.path.join(save_dir, 'generator.h5'))\n",
    "        self.discriminator.save(os.path.join(save_dir, 'discriminator.h5'))\n",
    "        self.supervisor.save(os.path.join(save_dir, 'supervisor.h5'))\n",
    "        \n",
    "        print(f\"\\nüíæ Models saved to {save_dir}\")\n",
    "\n",
    "print(\"‚úÖ TimeGAN class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201908b",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67055d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_synthetic_data(real_data, synthetic_data, feature_names):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of synthetic data quality.\n",
    "    \n",
    "    Metrics:\n",
    "    - Distributional similarity (KS test, JS divergence)\n",
    "    - Statistical properties (mean, std, skewness, kurtosis)\n",
    "    - Temporal properties (autocorrelation)\n",
    "    - Correlations between features\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SYNTHETIC DATA EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Distributional Similarity\n",
    "    print(\"\\nüìä 1. DISTRIBUTIONAL SIMILARITY (KS Test)\")\n",
    "    print(\"-\" * 80)\n",
    "    ks_stats = []\n",
    "    ks_pvalues = []\n",
    "    \n",
    "    for i, feat in enumerate(feature_names):\n",
    "        # Flatten sequences for distribution comparison\n",
    "        real_feat = real_data[:, :, i].flatten()\n",
    "        synth_feat = synthetic_data[:, :, i].flatten()\n",
    "        \n",
    "        # KS test\n",
    "        ks_stat, ks_pval = ks_2samp(real_feat, synth_feat)\n",
    "        ks_stats.append(ks_stat)\n",
    "        ks_pvalues.append(ks_pval)\n",
    "        \n",
    "        print(f\"   {feat:30s} - KS stat: {ks_stat:.4f}, p-value: {ks_pval:.4f}\")\n",
    "    \n",
    "    results['ks_stats'] = ks_stats\n",
    "    results['ks_pvalues'] = ks_pvalues\n",
    "    print(f\"\\n   Average KS statistic: {np.mean(ks_stats):.4f} (lower is better)\")\n",
    "    \n",
    "    # 2. Statistical Properties\n",
    "    print(\"\\nüìà 2. STATISTICAL PROPERTIES\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    stat_comparison = []\n",
    "    for i, feat in enumerate(feature_names):\n",
    "        real_feat = real_data[:, :, i].flatten()\n",
    "        synth_feat = synthetic_data[:, :, i].flatten()\n",
    "        \n",
    "        real_mean, synth_mean = np.mean(real_feat), np.mean(synth_feat)\n",
    "        real_std, synth_std = np.std(real_feat), np.std(synth_feat)\n",
    "        \n",
    "        stat_comparison.append({\n",
    "            'Feature': feat,\n",
    "            'Real_Mean': real_mean,\n",
    "            'Synth_Mean': synth_mean,\n",
    "            'Mean_Diff': abs(real_mean - synth_mean),\n",
    "            'Real_Std': real_std,\n",
    "            'Synth_Std': synth_std,\n",
    "            'Std_Diff': abs(real_std - synth_std)\n",
    "        })\n",
    "    \n",
    "    stat_df = pd.DataFrame(stat_comparison)\n",
    "    print(stat_df.to_string(index=False))\n",
    "    results['statistics'] = stat_df\n",
    "    \n",
    "    # 3. Autocorrelation\n",
    "    print(\"\\nüîÑ 3. TEMPORAL PROPERTIES (Autocorrelation at lag 1)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    autocorr_comparison = []\n",
    "    for i, feat in enumerate(feature_names[:5]):  # Check first 5 features\n",
    "        real_feat = real_data[:, :, i].flatten()\n",
    "        synth_feat = synthetic_data[:, :, i].flatten()\n",
    "        \n",
    "        real_autocorr = pd.Series(real_feat).autocorr(lag=1)\n",
    "        synth_autocorr = pd.Series(synth_feat).autocorr(lag=1)\n",
    "        \n",
    "        autocorr_comparison.append({\n",
    "            'Feature': feat,\n",
    "            'Real_ACF': real_autocorr,\n",
    "            'Synth_ACF': synth_autocorr,\n",
    "            'Diff': abs(real_autocorr - synth_autocorr)\n",
    "        })\n",
    "    \n",
    "    autocorr_df = pd.DataFrame(autocorr_comparison)\n",
    "    print(autocorr_df.to_string(index=False))\n",
    "    results['autocorrelation'] = autocorr_df\n",
    "    \n",
    "    # 4. Cross-correlations\n",
    "    if len(feature_names) > 1:\n",
    "        print(\"\\nüîó 4. FEATURE CORRELATIONS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Compute correlation matrices\n",
    "        real_corr = np.corrcoef(real_data.reshape(-1, len(feature_names)).T)\n",
    "        synth_corr = np.corrcoef(synthetic_data.reshape(-1, len(feature_names)).T)\n",
    "        \n",
    "        corr_diff = np.abs(real_corr - synth_corr)\n",
    "        print(f\"   Average correlation difference: {np.mean(corr_diff):.4f}\")\n",
    "        print(f\"   Max correlation difference: {np.max(corr_diff):.4f}\")\n",
    "        \n",
    "        results['correlation_diff'] = corr_diff\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ EVALUATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af1bad5",
   "metadata": {},
   "source": [
    "## 8. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_synthetic_data(real_data, synthetic_data, feature_names, ticker, save_path=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations comparing real and synthetic data.\n",
    "    \"\"\"\n",
    "    n_features_to_plot = min(4, len(feature_names))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_features_to_plot, 3, figsize=(20, 4*n_features_to_plot))\n",
    "    \n",
    "    if n_features_to_plot == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_features_to_plot):\n",
    "        feat_name = feature_names[i]\n",
    "        \n",
    "        # Extract feature data\n",
    "        real_feat = real_data[:, :, i].flatten()\n",
    "        synth_feat = synthetic_data[:, :, i].flatten()\n",
    "        \n",
    "        # Plot 1: Time series comparison\n",
    "        ax = axes[i, 0]\n",
    "        n_samples_to_plot = min(500, len(real_feat))\n",
    "        ax.plot(real_feat[:n_samples_to_plot], label='Real', alpha=0.7, linewidth=1)\n",
    "        ax.plot(synth_feat[:n_samples_to_plot], label='Synthetic', alpha=0.7, linewidth=1)\n",
    "        ax.set_title(f'{feat_name} - Time Series', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time Steps')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Distribution comparison\n",
    "        ax = axes[i, 1]\n",
    "        ax.hist(real_feat, bins=50, alpha=0.6, label='Real', density=True, color='blue')\n",
    "        ax.hist(synth_feat, bins=50, alpha=0.6, label='Synthetic', density=True, color='orange')\n",
    "        ax.set_title(f'{feat_name} - Distribution', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Q-Q plot\n",
    "        ax = axes[i, 2]\n",
    "        real_sorted = np.sort(real_feat)\n",
    "        synth_sorted = np.sort(synth_feat)\n",
    "        n_quantiles = min(len(real_sorted), len(synth_sorted))\n",
    "        ax.scatter(real_sorted[:n_quantiles], synth_sorted[:n_quantiles], alpha=0.3, s=1)\n",
    "        ax.plot([real_sorted.min(), real_sorted.max()], \n",
    "               [real_sorted.min(), real_sorted.max()], \n",
    "               'r--', linewidth=2, label='Perfect Match')\n",
    "        ax.set_title(f'{feat_name} - Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Real Quantiles')\n",
    "        ax.set_ylabel('Synthetic Quantiles')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{ticker} - Real vs Synthetic Data Comparison', \n",
    "                fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nüíæ Visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52eb4c7",
   "metadata": {},
   "source": [
    "## 9. Train TimeGAN on Selected Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a220966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first asset for demonstration\n",
    "ticker = TEST_ASSETS[0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TRAINING TIMEGAN ON {ticker}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load and prepare data\n",
    "data_scaled, scaler, feature_names = load_and_prepare_data(ticker, use_features=True)\n",
    "\n",
    "# Create sequences\n",
    "sequences = create_sequences(data_scaled, CONFIG['seq_len'])\n",
    "\n",
    "# Update config with actual number of features\n",
    "CONFIG['n_features'] = sequences.shape[2]\n",
    "\n",
    "print(f\"\\n‚úÖ Data prepared for TimeGAN training\")\n",
    "print(f\"   Sequences: {sequences.shape[0]:,}\")\n",
    "print(f\"   Sequence length: {sequences.shape[1]}\")\n",
    "print(f\"   Features: {sequences.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TimeGAN\n",
    "timegan = TimeGAN(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6427d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TimeGAN\n",
    "timegan.train(sequences, iterations=CONFIG['iterations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf77bc",
   "metadata": {},
   "source": [
    "## 10. Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce41b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic samples\n",
    "n_synthetic_samples = len(sequences)\n",
    "\n",
    "print(f\"\\nüé® Generating {n_synthetic_samples:,} synthetic samples...\")\n",
    "synthetic_data = timegan.generate_samples(n_synthetic_samples)\n",
    "\n",
    "print(f\"‚úÖ Synthetic data generated!\")\n",
    "print(f\"   Shape: {synthetic_data.shape}\")\n",
    "print(f\"   Range: [{synthetic_data.min():.3f}, {synthetic_data.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d134b",
   "metadata": {},
   "source": [
    "## 11. Evaluate Synthetic Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0329763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate synthetic data\n",
    "evaluation_results = evaluate_synthetic_data(sequences, synthetic_data, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "save_path = os.path.join(FIGURES_DIR, f'07_timegan_comparison_{ticker}.png')\n",
    "visualize_synthetic_data(sequences, synthetic_data, feature_names, ticker, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee9c9a",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "model_save_dir = os.path.join(MODELS_DIR, ticker)\n",
    "timegan.save_models(model_save_dir)\n",
    "\n",
    "# Save synthetic data\n",
    "synthetic_df = pd.DataFrame(\n",
    "    synthetic_data.reshape(-1, len(feature_names)),\n",
    "    columns=feature_names\n",
    ")\n",
    "synthetic_save_path = os.path.join(SYNTHETIC_DATA_DIR, f\"{ticker}_synthetic.csv\")\n",
    "synthetic_df.to_csv(synthetic_save_path, index=False)\n",
    "print(f\"\\nüíæ Synthetic data saved to {synthetic_save_path}\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_save_path = os.path.join(RESULTS_DIR, f'timegan_evaluation_{ticker}.csv')\n",
    "evaluation_results['statistics'].to_csv(eval_save_path, index=False)\n",
    "print(f\"üíæ Evaluation results saved to {eval_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61104f",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ TIMEGAN IMPLEMENTATION - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ COMPLETED TASKS:\")\n",
    "print(f\"   1. Implemented TimeGAN architecture with 5 networks\")\n",
    "print(f\"   2. Trained on {ticker} with {len(sequences):,} sequences\")\n",
    "print(f\"   3. Generated {n_synthetic_samples:,} synthetic samples\")\n",
    "print(f\"   4. Evaluated data quality with multiple metrics\")\n",
    "print(f\"   5. Saved models and synthetic data\")\n",
    "\n",
    "print(f\"\\nüìä KEY METRICS:\")\n",
    "avg_ks = np.mean(evaluation_results['ks_stats'])\n",
    "print(f\"   Average KS Statistic: {avg_ks:.4f} (lower is better, < 0.1 is good)\")\n",
    "\n",
    "mean_diff = evaluation_results['statistics']['Mean_Diff'].mean()\n",
    "std_diff = evaluation_results['statistics']['Std_Diff'].mean()\n",
    "print(f\"   Average Mean Difference: {mean_diff:.6f}\")\n",
    "print(f\"   Average Std Difference: {std_diff:.6f}\")\n",
    "\n",
    "if 'correlation_diff' in evaluation_results:\n",
    "    avg_corr_diff = np.mean(evaluation_results['correlation_diff'])\n",
    "    print(f\"   Average Correlation Difference: {avg_corr_diff:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ NEXT STEPS:\")\n",
    "print(f\"   1. Train TimeGAN on remaining assets: {', '.join(TEST_ASSETS[1:])}\")\n",
    "print(f\"   2. Implement Diffusion model for comparison\")\n",
    "print(f\"   3. Use synthetic data for downstream tasks (forecasting)\")\n",
    "print(f\"   4. Compare TimeGAN vs Diffusion vs Real data\")\n",
    "print(f\"   5. Evaluate data augmentation benefits\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TIMEGAN IMPLEMENTATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
